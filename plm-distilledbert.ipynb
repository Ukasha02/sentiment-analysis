{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7597281,"sourceType":"datasetVersion","datasetId":4422308}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-22T12:53:33.957197Z","iopub.execute_input":"2024-02-22T12:53:33.957520Z","iopub.status.idle":"2024-02-22T12:53:35.667530Z","shell.execute_reply.started":"2024-02-22T12:53:33.957494Z","shell.execute_reply":"2024-02-22T12:53:35.666511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing the training data\nimdb_data=pd.read_csv('/kaggle/input/preprocessing/train.csv')\nimdb_data_test=pd.read_csv('/kaggle/input/preprocessing/test.csv')\nprint(imdb_data.shape)\nimdb_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T13:16:09.861988Z","iopub.execute_input":"2024-02-21T13:16:09.862790Z","iopub.status.idle":"2024-02-21T13:16:11.817615Z","shell.execute_reply.started":"2024-02-21T13:16:09.862757Z","shell.execute_reply":"2024-02-21T13:16:11.816720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install vaderSentiment\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T18:05:11.372782Z","iopub.execute_input":"2024-02-24T18:05:11.373700Z","iopub.status.idle":"2024-02-24T18:05:25.089239Z","shell.execute_reply.started":"2024-02-24T18:05:11.373658Z","shell.execute_reply":"2024-02-24T18:05:25.088167Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vaderSentiment) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (2023.11.17)\nInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n# Download necessary NLTK data\nnltk.download('stopwords')\n\n# Data Preprocessing\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    stemmer = SnowballStemmer('english')\n    tokens = text.split()\n    filtered_tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('english')]\n    return ' '.join(filtered_tokens)\n\n# Assuming 'data.csv' is your dataset file with columns ['review', 'label']\ntrain_df = pd.read_csv('/kaggle/input/preprocessing/train.csv')\ntest_df = pd.read_csv('/kaggle/input/preprocessing/test.csv')\n\n\n\n# Preprocess reviews\ntrain_df['processed_review'] = train_df['review'].apply(preprocess_text)\ntest_df['processed_review'] = test_df['review'].apply(preprocess_text)\n\n# Initialize VADER\nanalyzer = SentimentIntensityAnalyzer()\n\n# Apply VADER to get sentiment scores for both datasets\ntrain_df['vader_score'] = train_df['processed_review'].apply(lambda review: analyzer.polarity_scores(review)['compound'])\ntest_df['vader_score'] = test_df['processed_review'].apply(lambda review: analyzer.polarity_scores(review)['compound'])\n\n# Prepare features and labels for the training set\nX_train = train_df['processed_review']\ny_train = train_df['sentiment']\nX_test = test_df['processed_review']\ny_test = test_df['sentiment']\n\n# Vectorize the text\ntfidf_vectorizer = TfidfVectorizer(max_features=50000)\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Add VADER scores as a feature\nX_train_vader = train_df['vader_score'].values.reshape(-1, 1)\nX_test_vader = test_df['vader_score'].values.reshape(-1, 1)\n\n# Combine TF-IDF features with VADER scores\nX_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_vader))\nX_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_vader))\n\n# Train the model on the training set\nmodel = RandomForestClassifier()\nmodel.fit(X_train_combined, y_train)\n\n# Predict on the test set\npredictions = model.predict(X_test_combined)\n\n# Evaluate and print the accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy on test set:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T18:05:29.059485Z","iopub.execute_input":"2024-02-24T18:05:29.059924Z","iopub.status.idle":"2024-02-24T18:38:49.221076Z","shell.execute_reply.started":"2024-02-24T18:05:29.059882Z","shell.execute_reply":"2024-02-24T18:38:49.220077Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nAccuracy on test set: 0.8533\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch\n\n# Load the tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the dataset\ndef tokenize_data(reviews, labels, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for review in reviews:\n        encoded_data = tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=max_length,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        input_ids.append(encoded_data['input_ids'])\n        attention_masks.append(encoded_data['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n\n    return input_ids, attention_masks, labels\n\n# Assuming imdb_data['review'] and imdb_data_test['review'] are your datasets\n# and imdb_data['sentiment'], imdb_data_test['sentiment'] are the labels\n# Convert labels to 0 and 1\ntrain_labels = [1 if label == 'positive' else 0 for label in imdb_data['sentiment']]\ntest_labels = [1 if label == 'positive' else 0 for label in imdb_data_test['sentiment']]\n\n# Tokenize training and validation datasets\nmax_length = 256 # You can adjust this\ntrain_input_ids, train_attention_masks, train_labels = tokenize_data(imdb_data['review'], train_labels, max_length)\ntest_input_ids, test_attention_masks, test_labels = tokenize_data(imdb_data_test['review'], test_labels, max_length)\n\n# Create DataLoader\nbatch_size = 16 # Adjust based on your GPU memory\n\ntrain_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T13:30:57.351554Z","iopub.execute_input":"2024-02-21T13:30:57.351856Z","iopub.status.idle":"2024-02-21T13:38:13.939728Z","shell.execute_reply.started":"2024-02-21T13:30:57.351831Z","shell.execute_reply":"2024-02-21T13:38:13.938858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport numpy as np\nimport random\n\n# Load DistilBERT pre-trained model\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels = 2, # Binary classification (positive, negative)\n    output_attentions = False,\n    output_hidden_states = False,\n)\n\n# Ensure model is running on GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Set up optimizer and scheduler for training\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nepochs = 10 # Number of training epochs, adjust as needed\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=total_steps\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T13:39:59.513853Z","iopub.execute_input":"2024-02-21T13:39:59.514313Z","iopub.status.idle":"2024-02-21T13:40:02.681529Z","shell.execute_reply.started":"2024-02-21T13:39:59.514265Z","shell.execute_reply":"2024-02-21T13:40:02.680555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# Function to calculate accuracy\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# Training loop\nfor epoch_i in range(0, epochs):\n    # Training\n    model.train()\n    total_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n        model.zero_grad()\n        \n        outputs = model(b_input_ids, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        \n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n    \n    avg_train_loss = total_loss / len(train_dataloader)\n    print(f\"Average training loss: {avg_train_loss}\")\n\n    # Evaluation\n    model.eval()\n    eval_accuracy = 0\n    nb_eval_steps = 0\n\n    for batch in test_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():\n            outputs = model(b_input_ids, attention_mask=b_input_mask)\n        \n        logits = outputs[0]\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n    \n    print(f\"Accuracy: {eval_accuracy/nb_eval_steps}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T13:40:15.922180Z","iopub.execute_input":"2024-02-21T13:40:15.922704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}