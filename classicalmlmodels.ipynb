{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7597281,"sourceType":"datasetVersion","datasetId":4422308}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Load the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport spacy\nimport re,string,unicodedata\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom textblob import TextBlob\nfrom textblob import Word\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\nimport os\nprint(os.listdir(\"../input\"))\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-24T20:12:30.803639Z","iopub.execute_input":"2024-02-24T20:12:30.804239Z","iopub.status.idle":"2024-02-24T20:12:42.656209Z","shell.execute_reply.started":"2024-02-24T20:12:30.804207Z","shell.execute_reply":"2024-02-24T20:12:42.655254Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['preprocessing']\n","output_type":"stream"}]},{"cell_type":"code","source":"#importing the training data\nimdb_data=pd.read_csv('/kaggle/input/preprocessing/train.csv')\nimdb_data_test=pd.read_csv('/kaggle/input/preprocessing/test.csv')\nprint(imdb_data.shape)\nimdb_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T20:13:23.619270Z","iopub.execute_input":"2024-02-24T20:13:23.619864Z","iopub.status.idle":"2024-02-24T20:13:25.076755Z","shell.execute_reply.started":"2024-02-24T20:13:23.619835Z","shell.execute_reply":"2024-02-24T20:13:25.075762Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(30000, 2)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  SAPS AT SEA <br /><br />Aspect ratio: 1.37:1<b...  negative\n1  If you want mindless action, hot chicks and a ...  positive\n2  \"The Woman in Black\" is easily one of the cree...  positive\n3  I can barely find the words to describe how mu...  negative\n4  What's in here ?! Let me tell you. It's the pr...  negative\n5  This is the story of a maniac cop who, for som...  negative\n6  Before I continue forth with the new millenniu...  positive\n7  When Rodney Dangerfield is on a roll, he's hil...  negative\n8  Prom Night is shot with the artistic eye someo...  negative\n9  \"Destroy All Planets\" winds up settling for 'd...  negative","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SAPS AT SEA &lt;br /&gt;&lt;br /&gt;Aspect ratio: 1.37:1&lt;b...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If you want mindless action, hot chicks and a ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"The Woman in Black\" is easily one of the cree...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I can barely find the words to describe how mu...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What's in here ?! Let me tell you. It's the pr...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>This is the story of a maniac cop who, for som...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Before I continue forth with the new millenniu...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>When Rodney Dangerfield is on a roll, he's hil...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Prom Night is shot with the artistic eye someo...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>\"Destroy All Planets\" winds up settling for 'd...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_text(text, method='stemming'):\n    # Define the tokenizer\n    tokenizer = ToktokTokenizer()\n    \n    # Define stopword list\n    stopword_list = nltk.corpus.stopwords.words('english')\n    \n    # Removing HTML tags\n    soup = BeautifulSoup(text, \"html.parser\")\n    text = soup.get_text()\n    \n    # Removing text inside square brackets\n    text = re.sub('\\[[^]]*\\]', '', text)\n    \n    # Define function for removing special characters\n    def remove_special_characters(text, remove_digits=True):\n        pattern = r'[^a-zA-z0-9\\s]'\n        text = re.sub(pattern, '', text)\n        return text\n    \n    # Remove special characters\n    text = remove_special_characters(text)\n    \n    # Tokenization\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    \n    # Stemming or Lemmatization\n    if method == 'stemming':\n        stemmer = PorterStemmer()\n        tokens = [stemmer.stem(token) for token in tokens]\n    elif method == 'lemmatization':\n        lemmatizer = WordNetLemmatizer()\n        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n    \n    # Removing the stopwords\n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    \n    # Reconstruct the text\n    text = ' '.join(filtered_tokens)\n    \n    return text\n\n# Example usage\nmethod = 'lemmatization' # or 'stemming'\nimdb_data['review'] = imdb_data['review'].apply(lambda x: preprocess_text(x, method=method))\nimdb_data_test['review'] = imdb_data_test['review'].apply(lambda x: preprocess_text(x, method=method))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T21:02:56.276884Z","iopub.execute_input":"2024-02-24T21:02:56.277266Z","iopub.status.idle":"2024-02-24T21:04:21.578693Z","shell.execute_reply.started":"2024-02-24T21:02:56.277237Z","shell.execute_reply":"2024-02-24T21:04:21.577882Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#Count vectorizer for bag of words\ncv=CountVectorizer(max_features = 60000, ngram_range = (1,3), max_df = 0.8, min_df = 3)\n#transformed train reviews\ncv_train_reviews=cv.fit_transform(imdb_data['review'])\ncv_test_reviews=cv.transform(imdb_data_test['review'])\n\n\nprint('BOW_cv_train:',cv_train_reviews.shape)\n\n\n\n\nprint('BOW_cv_test:',cv_test_reviews.shape)\n\n#vocab=cv.get_feature_names()-toget feature names","metadata":{"execution":{"iopub.status.busy":"2024-02-24T21:04:32.892833Z","iopub.execute_input":"2024-02-24T21:04:32.893241Z","iopub.status.idle":"2024-02-24T21:05:17.941767Z","shell.execute_reply.started":"2024-02-24T21:04:32.893211Z","shell.execute_reply":"2024-02-24T21:05:17.940760Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"BOW_cv_train: (30000, 60000)\nBOW_cv_test: (20000, 60000)\n","output_type":"stream"}]},{"cell_type":"code","source":"tv=TfidfVectorizer(max_features = 50000, ngram_range = (1,3))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(imdb_data['review'])\ntv_test_reviews=tv.transform(imdb_data_test['review'])\n\n\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T20:49:51.688617Z","iopub.execute_input":"2024-02-24T20:49:51.689393Z","iopub.status.idle":"2024-02-24T20:50:37.550286Z","shell.execute_reply.started":"2024-02-24T20:49:51.689358Z","shell.execute_reply":"2024-02-24T20:50:37.549267Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Tfidf_train: (30000, 50000)\nTfidf_test: (20000, 50000)\n","output_type":"stream"}]},{"cell_type":"code","source":"#labeling the sentient data\nlb=LabelBinarizer()\n#transformed sentiment data\nsentiment_data=lb.fit_transform(imdb_data['sentiment'])\nsentiment_data_test=lb.transform(imdb_data_test['sentiment'])\nprint(sentiment_data.shape)\nprint(sentiment_data_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T21:05:27.921528Z","iopub.execute_input":"2024-02-24T21:05:27.922240Z","iopub.status.idle":"2024-02-24T21:05:28.087911Z","shell.execute_reply.started":"2024-02-24T21:05:27.922208Z","shell.execute_reply":"2024-02-24T21:05:28.087054Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"(30000, 1)\n(20000, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Spliting the sentiment data\nsentiment_data=sentiment_data[:30000]\nsentiment_data_test=sentiment_data_test[:20000]\n\nprint(sentiment_data)\nprint(sentiment_data_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T21:05:30.219558Z","iopub.execute_input":"2024-02-24T21:05:30.220395Z","iopub.status.idle":"2024-02-24T21:05:30.226779Z","shell.execute_reply.started":"2024-02-24T21:05:30.220359Z","shell.execute_reply":"2024-02-24T21:05:30.225573Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"[[0]\n [1]\n [1]\n ...\n [1]\n [0]\n [0]]\n[[1]\n [1]\n [0]\n ...\n [0]\n [1]\n [0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Initialize the KNN model\nknn = KNeighborsClassifier()\n\n# Start timing\nstart_time = time.time()\n\n# Fitting the KNN model for bag of words\nknn_bow = knn.fit(tv_train_reviews, sentiment_data)\nprint(knn_bow)\n\n# Model prediction\nknn_bow_predict = knn.predict(tv_test_reviews)\n\n# End timing\nend_time = time.time()\n\n# Calculate the elapsed time\nelapsed_time = end_time - start_time\n\nprint(knn_bow_predict)\nprint(f\"Time taken for model to run: {elapsed_time} seconds\")\n\n# Accuracy score for bag of words\nknn_bow_score = accuracy_score(sentiment_data_test, knn_bow_predict)\nprint(\"knn_bow_score :\", knn_bow_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:49:48.533776Z","iopub.execute_input":"2024-02-20T19:49:48.534482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Initialize the XGBoost classifier model\nxgb = XGBClassifier(\n    n_estimators=1000,    # Start with a moderate number and increase if needed\n    learning_rate=0.1,   # Lower learning rate for a start\n    max_depth=7,         # Start with a moderate depth\n    min_child_weight=1,  # Default value\n    gamma=0.1,           # Slight regularization\n    subsample=0.8,       # 80% of data to grow trees prevents overfitting\n    colsample_bytree=0.8, # 80% of features used\n    objective='binary:logistic', # Since the dataset is balanced and binary classification\n    use_label_encoder=False, # To avoid warning since XGBClassifier uses a label encoder internally by default\n    eval_metric='logloss' # Evaluation metric for binary classification\n)\n\n# Fitting the XGBoost model for bag of words\nprint(\"Training XGBoost model...\")\nstart_train_time = time.time()\nxgb_bow = xgb.fit(cv_train_reviews, sentiment_data)\nend_train_time = time.time()\nprint(\"Training completed.\")\n\n# Start timing for prediction\nstart_predict_time = time.time()\n\n# Model prediction\nxgb_bow_predict = xgb.predict(cv_test_reviews)\n\n# End timing for prediction\nend_predict_time = time.time()\n\n# Calculate the elapsed time\ntrain_elapsed_time = end_train_time - start_train_time\npredict_elapsed_time = end_predict_time - start_predict_time\n\nprint(xgb_bow_predict)\nprint(f\"Time taken to train the model: {train_elapsed_time} seconds\")\nprint(f\"Time taken for model to predict: {predict_elapsed_time} seconds\")\n\n# Accuracy score for bag of words\nxgb_bow_score = accuracy_score(sentiment_data_test, xgb_bow_predict)\nprint(\"xgb_bow_score:\", xgb_bow_score)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T21:30:11.879799Z","iopub.execute_input":"2024-02-24T21:30:11.880787Z","iopub.status.idle":"2024-02-24T21:31:50.752516Z","shell.execute_reply.started":"2024-02-24T21:30:11.880741Z","shell.execute_reply":"2024-02-24T21:31:50.751646Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Training XGBoost model...\nTraining completed.\n[1 1 0 ... 0 1 0]\nTime taken to train the model: 97.90205764770508 seconds\nTime taken for model to predict: 0.9559218883514404 seconds\nxgb_bow_score: 0.88725\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Initialize the Multinomial Naive Bayes model\nmnb = MultinomialNB()\n\n# Fitting the MNB model for bag of words\nmnb_bow = mnb.fit(tv_train_reviews, sentiment_data)\nprint(mnb_bow)\n\n# Start timing\nstart_time = time.time()\n\n# Model prediction\nmnb_bow_predict = mnb.predict(tv_test_reviews)\n\n# End timing\nend_time = time.time()\n\n# Calculate the elapsed time\nelapsed_time = end_time - start_time\n\nprint(mnb_bow_predict)\nprint(f\"Time taken for model to run: {elapsed_time} seconds\")\n\n# Accuracy score for bag of words\nmnb_bow_score = accuracy_score(sentiment_data_test, mnb_bow_predict)\nprint(\"mnb_bow_score :\", mnb_bow_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:48:13.736679Z","iopub.execute_input":"2024-02-20T19:48:13.737113Z","iopub.status.idle":"2024-02-20T19:48:13.779007Z","shell.execute_reply.started":"2024-02-20T19:48:13.737082Z","shell.execute_reply":"2024-02-20T19:48:13.777972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Initialize the Random Forest model\nrf = RandomForestClassifier()\n\n# Start timing\nstart_time = time.time()\n\n# Fitting the RF model for bag of words\nrf_bow = rf.fit(tv_train_reviews, sentiment_data)\nprint(rf_bow)\n\n# Model prediction\nrf_bow_predict = rf.predict(tv_test_reviews)\n\n# End timing\nend_time = time.time()\n\n# Calculate the elapsed time\nelapsed_time = end_time - start_time\n\nprint(rf_bow_predict)\nprint(f\"Time taken for model to run: {elapsed_time} seconds\")\n\n# Accuracy score for bag of words\nrf_bow_score = accuracy_score(sentiment_data_test, rf_bow_predict)\nprint(\"rf_bow_score :\", rf_bow_score)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T19:48:16.994755Z","iopub.execute_input":"2024-02-20T19:48:16.995163Z","iopub.status.idle":"2024-02-20T19:49:27.933785Z","shell.execute_reply.started":"2024-02-20T19:48:16.995133Z","shell.execute_reply":"2024-02-20T19:49:27.932753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}